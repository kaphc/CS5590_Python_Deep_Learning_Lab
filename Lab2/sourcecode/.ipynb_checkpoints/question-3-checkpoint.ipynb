{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:\n",
    "1. pandas library - which is mainly used for data manipulation and analysis.\n",
    "2. sklearn library - helps us to implement machine learning techniques.\n",
    "3. keras library - consists of functions and programs related to neural network.\n",
    "4. re library - operations related to regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, SpatialDropout1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras import optimizers\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Dataset:\n",
    "1. Reading csv file using pandas library.\n",
    "2. Pass filename and encoding (encoding which is used for UTF while reading/writing example: 'utf-8') as parameter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('spam.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Dataset:\n",
    ".head and .info displays the dataset and information about the dataset respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unecessary columns:\n",
    "1. Unecessary columns are dropped using .drop of method.\n",
    "2. Pass list of column names to be dropped, index (0 to drop from index or 1 to drop from columns), inplace (true, for doing operation inplace or else return none)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check drop action:\n",
    "Check whether the unecessary features are dropped from the dataframe y viewing the dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing:\n",
    "1. Use .strip to remove empty charecter from both the sides.\n",
    "2. Use .lower function to change all the string to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['spam_or_ham']=dataset.spam_or_ham.str.strip()\n",
    "dataset['message']=dataset.message.str.strip()\n",
    "dataset['message']=dataset.message.str.lower()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count categories:\n",
    "Use .value_counts function to count number of datapoints in each of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.spam_or_ham.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization:\n",
    "1. Tokenization chops the string into pieces called tokens.\n",
    "2. For example: \n",
    "input : \"Hi python, this is my last assignment\"\n",
    "output : [\"hi\",\"python\",\"this\",\"is\",\"my\",\"last\",\"assignment\"]\n",
    "3. Create tokenizer model with 1500 as num_word and split the string by the charecter ' '.\n",
    "4. fit on the dataset.\n",
    "5. make the texts to sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_number_of_features = 1500\n",
    "tokenizer = Tokenizer(num_words=maximum_number_of_features, split=' ')\n",
    "tokenizer.fit_on_texts(dataset['message'].values)\n",
    "X = tokenizer.texts_to_sequences(dataset['message'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the sequence to 2D Numpy array:\n",
    "Use pad_sequences from keras to change the sequence to 2D Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "X = pad_sequences(X)\n",
    "print(\"after applying pad_sequences\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make categorical variable:\n",
    "1. Create a LabelEncoder model.\n",
    "2. Use fit_transform to fit the categorical feature 'spam_or_ham to chenge it to categorical variable.\n",
    "3. LabelEncoder makes the classes from 0 to total class-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "integer_encoded = labelencoder.fit_transform(dataset['spam_or_ham'])\n",
    "Y = to_categorical(integer_encoded)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Dataset:\n",
    "1. Use train_test_split() function to split the into four that is to categorical feature for train and test (Y_train and Y_test) and othe features (dependent features) for train and test (X_train and X_test).\n",
    "2. Make test size to 33%.\n",
    "3. Use random state to randomize the datapoints\n",
    "4. Print the shape of the splitted datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Model training:\n",
    "1. Create a Sequential model where sequential model where sequential model is adding to several layers to the model.\n",
    "2. Add several layers like Embedding, Conv1D with MaxPooling1D and SpatialDropout1D\n",
    "3. At last flatten the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(1500, 128, dropout=0.2, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model:\n",
    "1. Fit the trained model on the splitted dataset\n",
    "2. Fit the model for 16 epochs(number of times), batch_size(number of datapoints taken for each step of fitting), validation_data for doing validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=16, batch_size=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Accuracy of the model:\n",
    "Find the accuracy using model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(X_test, Y_test, verbose=2, batch_size=100)\n",
    "print(score)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
